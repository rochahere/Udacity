# Project 1: Data Modeling with Postgres

User activity data will be model to create a database and ETL pipeline in Postgres for a music streaming app. Fact and Dimension tables will be define and data will be inserted into new tables.

## Introduction

A startup called Sparkify want to analyze the data they have been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to.

## Project Description

Apply data modeling with Postgres and build an ETL pipelines using Python.

## Project Datasets

### Song Dataset
Subset of real data from the Million Song Dataset. Each file is on JSON format and contains metadata about a song.

### Log Dataset
This dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above.

## Project Instructions

### Schema for the Songs Play Analysis

#### Fact Table
> 1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
    * songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
#### Dimension Tables
> 2. **users** - users in the app
    * user_id, first_name, last_name, gender, level
> 3. **songs** - songs in music database
    * song_id, title, artist_id, year, duration
> 4. **artists** - artists in music database
    * artist_id, name, location, latitude, longitude
> 5. **time** - timestamps of records in songplays broken down into specific units
    * start_time, hour, day, week, month, year, weekday

## Project Steps

### Create Tables
1. Write **CREATE** statements in **sql_queries.py** to create each table.
2. Write **DROP** statements in **sql_queries.py** to drop each table if it exists.
3. Run **create_tables.py** to create your database and tables.
4. Run **test.ipynb** to confirm the creation of your tables with the correct columns. Make sure to click **"Restart kernel"** to close the connection to the database after running this notebook.

### Build ETL Processes
Follow instructions in the **etl.ipynb** notebook to develop ETL processes for each table. At the end of each table section, or at the end of the notebook, run **test.ipynb** to confirm that records were successfully inserted into each table. Remember to rerun **create_tables.py** to reset your tables before each time you run this notebook.

### Build ETL Pipeline
Use what you have completed in **etl.ipynb** to complete **etl.py**, where you'll process the entire datasets. Remember to run **create_tables.py** before running **etl.py** to reset your tables. Run **test.ipynb** to confirm your records were successfully inserted into each table.
